{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afa128b2",
   "metadata": {},
   "source": [
    "# Assignment 2 Lab\n",
    "## Cost Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc53ec0",
   "metadata": {},
   "source": [
    "## Spring 2024 (February 22, 2024)\n",
    "## DSSA5104"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54e9f53-6936-407f-b598-42922f5b7ed0",
   "metadata": {},
   "source": [
    "### <font color='blue'> Instructions</font>\n",
    "Here is another Jupyter Notebook to work with Cost (or Loss) functions. However, I have not completed all the code. Please complete the code so that it works, and also answer the question at the bottom of the notebook.\n",
    "\n",
    "I hope this assignment will not be too difficult for anyone. So study what it is doing so that you really understand why we use these cost functions.\n",
    "\n",
    "For additional information, see pp.64-73 of our textbook Make Your Own Neural Network (2016) by Tariq Rashid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15874bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required Python Libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d92fb2f",
   "metadata": {},
   "source": [
    "This notebook is an initial examination of cost functions. A cost function can be any function that outputs a scalar that quantifies the error of your neural networkâ€™s performance.\n",
    "\n",
    "Say we have output data of the expected results and actual results from a Neural Network. \n",
    "Although we should probably read it in as a Pandas dataframe, for this example we will just use a numpy matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59d43214",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#loss = np.random.normal(size = (100,2))\n",
    "#np.savetxt(\"assignment3.csv\", loss, delimiter=\",\", fmt='%1.3f')\n",
    "loss = np.loadtxt(\"assignment3.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ae8b97",
   "metadata": {},
   "source": [
    "We know that the first column are the expected results, and the second column are the actual results. Remember that Python is zero-based. So column 0 is expected and column 1 is actual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6d656b",
   "metadata": {},
   "source": [
    "At a basic level, we want to know the difference between the expected results and the actual results. We can compute the sum of the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d1d6944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6319999999999988"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(loss[:,0] - loss[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754fd0fa",
   "metadata": {},
   "source": [
    "And then we can take the average to get the average difference across all observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e41fd95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016319999999999987"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(loss[:,0] - loss[:,1]) / loss[:,0].size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336336aa",
   "metadata": {},
   "source": [
    "We could save the result as the average of the differences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "729d06f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = loss[:,0].size\n",
    "avgDiff = sum(loss[:,0] - loss[:,1]) / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3893694",
   "metadata": {},
   "source": [
    "That looks really good, except for the fact that the negatives are canceling out the positive errors! If we look at the data (print(loss)), we can see that most actual values are off much more than our avgDiff metric!\n",
    "\n",
    "We need a better metric for the loss (or cost). We need a cost function that can be written as a function of the outputs from the neural network, and we will assume our cost functions meet that assumption. \n",
    "\n",
    "Since we are going to be working with these two columns of data, let us make it easy on ourselves and set them to variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db486cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = loss[:,0]\n",
    "actual = loss[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4488a4df",
   "metadata": {},
   "source": [
    "We know there are plenty of cost functions, and we reviewed four of them in class. The Mean Squared Error (MSE), the Standard Error (SE), the Root Mean Square error (RMS), and the Sum of Squared Errors (SSE). Refer to the class slides (or any reference you like) and compute the MSE, SE, RMS, and SSE. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd03428-ada9-442b-8bad-a2656c341b8a",
   "metadata": {},
   "source": [
    "## <font color='red'>1. Complete the code in the following block</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1aeb87fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squared Errors: [4.6958890e+00 2.5600000e-02 2.2801000e-02 1.0732960e+00 1.0506250e+00\n",
      " 1.4592640e+00 5.4760000e-01 1.3876840e+00 6.6259600e-01 2.3225760e+00\n",
      " 2.5000000e-05 1.0201000e+00 9.1393600e-01 2.1520890e+00 7.9566400e-01\n",
      " 4.7698560e+00 1.0650240e+00 5.1552400e-01 7.3657960e+00 2.1667840e+00\n",
      " 5.8081000e-02 5.2900000e-02 2.6863210e+00 9.3025000e-02 1.2144040e+00\n",
      " 6.1622500e-01 1.4592640e+00 1.9184400e-01 3.9312900e-01 4.4859240e+00\n",
      " 1.6563690e+00 5.9907600e-01 5.2670250e+00 6.7600000e-02 1.0261690e+00\n",
      " 2.9484900e-01 1.6080100e-01 2.1902400e-01 1.4424010e+00 5.3824000e-02\n",
      " 5.1984000e-02 2.6503840e+00 1.5276960e+00 1.7875690e+00 1.3806250e+00\n",
      " 1.0125124e+01 3.0206440e+00 4.2107040e+00 5.4908100e-01 1.1384890e+00\n",
      " 1.2904960e+00 3.1011210e+00 3.0136960e+00 9.3636000e-02 7.2199690e+00\n",
      " 2.2710490e+00 2.8561000e-02 1.6563690e+00 1.5681600e+01 2.2657600e-01\n",
      " 2.0903184e+01 2.1996100e-01 6.8227600e-01 6.7732900e-01 8.7422500e-01\n",
      " 1.0609000e-02 2.1874410e+00 1.6646400e-01 4.7089000e-02 7.6176000e-02\n",
      " 8.5849000e-02 6.9537690e+00 4.6569640e+00 3.0470400e-01 1.1815690e+00\n",
      " 8.1902500e-01 6.2884900e-01 7.3273600e-01 3.5721000e-02 1.6078240e+00\n",
      " 5.8216900e-01 2.1462250e+00 8.0485690e+00 7.1824000e+00 8.3905600e-01\n",
      " 1.2321000e-02 3.9187600e-01 4.2510400e-01 5.4802810e+00 1.1383876e+01\n",
      " 2.1904000e+00 3.4456900e-01 1.6000000e-01 1.6224784e+01 3.1011210e+00\n",
      " 3.1024900e-01 2.3716000e-02 5.0445160e+00 4.0844410e+00 2.7822240e+00]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the squared errors\n",
    "    # Subtract everything by the first column by the second column\n",
    "        # Then square the difference\n",
    "squared_errors = (loss[:, 0] - loss[:, 1]) ** 2\n",
    "\n",
    "print(\"Squared Errors:\", squared_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a774d29f-ff6a-48fb-8499-621195d54b62",
   "metadata": {},
   "source": [
    "## <font color='red'>2. Complete the code in the following block</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "554296da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Squared Errors: 230.686018\n"
     ]
    }
   ],
   "source": [
    "# Calculate the SSE by getting the sum of squared errors\n",
    "sse = np.sum(squared_errors)\n",
    "print(\"Sum of Squared Errors:\", sse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ebe12f-1e1f-4b53-be9b-ef421495588f",
   "metadata": {},
   "source": [
    "## <font color='red'>3. Complete the code in the following block</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7e4222b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 2.3068601799999997\n"
     ]
    }
   ],
   "source": [
    "# Calculate the MSE by getting the mean of squared errors\n",
    "mse = np.mean(squared_errors)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b59d5b-f3e1-4e77-b183-95c330a34bea",
   "metadata": {},
   "source": [
    "## <font color='red'>4. Complete the code in the following block</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d92e8f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 1.5188351391773893\n"
     ]
    }
   ],
   "source": [
    "# Calculate the RMS by square rooting the mse\n",
    "rms = np.sqrt(mse)\n",
    "print(\"Root Mean Squared Error:\", rms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a2fd38",
   "metadata": {},
   "source": [
    "While these are the most popular, there is no scientific way to determine what cost function is the best (although some may be inappropriate for your use). It is more trial and error to find the one you like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a3986d",
   "metadata": {},
   "source": [
    "Depending on your data and model, there are many other cost functions available (e.g. Cross-Entropy, Exponential, Hellinger Distance, Kullback-Leibler Divergence). See https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications. Personally I have mainly used the four common ones computed above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41557a6",
   "metadata": {},
   "source": [
    "## <font color='red'>5. Question</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6f57a4",
   "metadata": {},
   "source": [
    "Write a one-paragraph explanation of why we need a cost (or loss) function at all. You can insert your paragraph into this Notebook or attach it separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd155aa5-8ab3-4ed1-9edb-d8424784bd23",
   "metadata": {},
   "source": [
    "In the realm of machine learning, a cost (or loss) function is a fundamental tool to see if models are working properly and to make them better. The function allows for a quantitative measure for how good a model is working by getting the difference between its predicted outputs and its actual values. Therefore, by calculating this discrepancy, it allows for an optimization algorithms during the training process, which again allows for the modelâ€™s parameters to be adjusted in order to minimize errors. Without this metric, analyzing a modelâ€™s performance and making informed decisions would be extremely hard.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    ".."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7ed8c8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
